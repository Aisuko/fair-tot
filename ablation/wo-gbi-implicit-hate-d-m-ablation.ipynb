{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":271710136,"sourceType":"kernelVersion"},{"sourceId":271950668,"sourceType":"kernelVersion"},{"sourceId":271954096,"sourceType":"kernelVersion"},{"sourceId":271954147,"sourceType":"kernelVersion"},{"sourceId":271997047,"sourceType":"kernelVersion"},{"sourceId":271998226,"sourceType":"kernelVersion"},{"sourceId":272007741,"sourceType":"kernelVersion"},{"sourceId":272008020,"sourceType":"kernelVersion"},{"sourceId":272287529,"sourceType":"kernelVersion"},{"sourceId":272290955,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U wurun","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T02:54:58.634439Z","iopub.execute_input":"2025-11-01T02:54:58.635221Z","iopub.status.idle":"2025-11-01T02:55:02.517828Z","shell.execute_reply.started":"2025-11-01T02:54:58.635189Z","shell.execute_reply":"2025-11-01T02:55:02.516563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast, math\nimport asyncio\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom wurun import Wurun\nfrom typing import List, Any, Mapping\nfrom pydantic import BaseModel, Field, field_validator, ValidationError\n\nclass DatasetRow(BaseModel):\n    target: str                 # e.g., \"immigrants\"\n    enr_parsed: float           # e.g., 0.73\n    sigma_q_e: List[float]      # e.g., [0.12, 0.10, -0.03, 0.05]\n    theta_cf: float             # e.g., 0.135469\n\n    # Coerce \"sigma_q_e\" (accept stringified list)\n    @field_validator(\"sigma_q_e\", mode=\"before\")\n    @classmethod\n    def _coerce_sigma(cls, v: Any):\n        if isinstance(v, str):\n            v = ast.literal_eval(v)  # \"[0.1, 0.2]\" -> [0.1, 0.2]\n        if not isinstance(v, (list, tuple)):\n            raise TypeError(\"sigma_q_e must be a list of floats.\")\n        out = [float(x) for x in v]\n        if not out or any(not math.isfinite(x) for x in out):\n            raise ValueError(\"sigma_q_e must be non-empty and all finite.\")\n        return out\n\n    # Coerce floats that may come as strings\n    @field_validator(\"enr_parsed\", \"theta_cf\", mode=\"before\")\n    @classmethod\n    def _coerce_float(cls, v: Any):\n        return float(v)\n\n    # Extra check for theta_cf (>= 0)\n    @field_validator(\"theta_cf\")\n    @classmethod\n    def _theta_nonneg(cls, v: float):\n        if v < 0 or not math.isfinite(v):\n            raise ValueError(\"theta_cf must be a non-negative, finite float.\")\n        return v\n\n    # Optional: trim target\n    @field_validator(\"target\", mode=\"before\")\n    @classmethod\n    def _trim_target(cls, v: Any):\n        s = str(v).strip()\n        if not s:\n            raise ValueError(\"target must be a non-empty string.\")\n        return s\n\n\nREQUIRED = [\"target\", \"enr_parsed\", \"sigma_q_e\", \"theta_cf\"]\n\n# ---- Notebook-friendly helpers ----\ndef validate_dataframe(df: pd.DataFrame) -> list[DatasetRow]:\n    \"\"\"Validate df rows and return a list of DatasetRow objects.\"\"\"\n    missing = [c for c in REQUIRED if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing required columns: {missing}\")\n\n    objs, errors = [], []\n    for i, row in df.iterrows():\n        try:\n            objs.append(DatasetRow.model_validate(row.to_dict()))\n        except ValidationError as e:\n            errors.append((i, e))\n\n    if errors:\n        lines = []\n        for i, e in errors[:10]:\n            lines.append(f\"row={i}: {e.errors()}\")\n        more = f\" ... and {len(errors)-10} more rows\" if len(errors) > 10 else \"\"\n        raise ValueError(\"Validation failed:\\n\" + \"\\n\".join(lines) + more)\n\n    return objs\n\n\ndef coerce_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Return a copy of df with the four columns coerced/validated by Pydantic.\"\"\"\n    out = df.copy()\n    objs = validate_dataframe(out)  # validates & coerces\n    for idx, obj in zip(out.index, objs):\n        out.at[idx, \"target\"] = obj.target\n        out.at[idx, \"enr_parsed\"] = obj.enr_parsed\n        out.at[idx, \"sigma_q_e\"] = obj.sigma_q_e\n        out.at[idx, \"theta_cf\"] = obj.theta_cf\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T02:55:02.520276Z","iopub.execute_input":"2025-11-01T02:55:02.520595Z","iopub.status.idle":"2025-11-01T02:55:02.540229Z","shell.execute_reply.started":"2025-11-01T02:55:02.520549Z","shell.execute_reply":"2025-11-01T02:55:02.539090Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = \"toxigen\"\n\nmatch dataset:\n    case \"toxigen\":\n        df = pd.read_pickle(\"/kaggle/input/implicit-hate-speech-on-toxigen/ready_data.pkl\")\n    case \"offenslang\":\n        df = pd.read_pickle(\"/kaggle/input/implicit-speech-on-offensive-slang/ready_data.pkl\")\n    case \"latent_hatred\":\n        df = pd.read_pickle(\"/kaggle/input/implicit-hate-detection/ready_data.pkl\")    \n    case _:\n        raise ValueError(f\"Unknown dataset name: {dataset}\")\n\n\ndf.rename(columns={\"target_group\": \"target\"}, inplace=True)\n\ndf = coerce_dataframe(df)\ndf.drop(columns=[\"enr\",\"enr_parsed\"], inplace=True)\n\ndf[\"sigma_q_e\"] = df.apply(\n    lambda r: (\n        [float(x) for x in r[\"cs_q_e_parsed\"]]\n        if isinstance(r[\"cs_q_e_parsed\"], list)\n        and all(isinstance(x, (int, float)) for x in r[\"cs_q_e_parsed\"])\n        else None\n    ),\n    axis=1\n)\n\ndf[\"theta_cf\"] = df[\"sigma_q_e\"].apply(lambda xs: np.var(xs)) # var function with ddof=0 by default\ndf[\"theta_cf\"] = pd.to_numeric(df[\"theta_cf\"], errors=\"coerce\")\ndf[\"theta_cf\"] = df[\"theta_cf\"].replace({np.inf: np.nan, -np.inf: np.nan})\ndf = df.dropna(subset=[\"theta_cf\"]).copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T02:55:02.541340Z","iopub.execute_input":"2025-11-01T02:55:02.541692Z","iopub.status.idle":"2025-11-01T02:55:02.589616Z","shell.execute_reply.started":"2025-11-01T02:55:02.541662Z","shell.execute_reply":"2025-11-01T02:55:02.588900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\nfrom matplotlib.ticker import FuncFormatter\n\n# --- 1) Prepare the dataframe ---\n# Make sure we have a unique ID for each row\nif \"row_id\" not in df.columns:\n    df = df.reset_index(names=\"row_id\")  # safe to run multiple times\n\n# Ensure sigma_q_e is a list (in case it's a string like \"[0.1, 0.2, 0.3, 0.4]\")\ndf[\"sigma_q_e\"] = df[\"sigma_q_e\"].apply(\n    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n)\n\n# Define your fixed entity list (lowercase)\nentity_list = df[\"target\"].str.lower().unique().tolist()\nprint(entity_list)\n\n# Assign this same list to every row\ndf[\"entity_list\"] = [entity_list] * len(df)\n\n# --- 2) Explode to long format ---\ndf_long = (\n    df.loc[:, [\"row_id\", \"theta_cf\", \"sigma_q_e\", \"entity_list\"]]\n      .explode([\"sigma_q_e\", \"entity_list\"], ignore_index=False)\n      .rename(columns={\"sigma_q_e\": \"sigma\", \"entity_list\": \"entity\"})\n      .reset_index(drop=True)\n)\n\n# --- 3) Compute entity-level stats ---\nentity_stats = (\n    df_long.groupby(\"entity\", as_index=False).agg(\n        n=(\"sigma\", \"size\"),\n        mu_sigma=(\"sigma\", \"mean\"),\n        var_sigma=(\"sigma\", \"var\"),\n        mad_sigma=(\"sigma\", lambda x: np.median(np.abs(x - np.median(x)))),\n        q95_abs=(\"sigma\", lambda x: np.quantile(np.abs(x), 0.95)),\n        max_abs=(\"sigma\", lambda x: np.max(np.abs(x))),\n    )\n)\n\neps = 1e-8\nentity_stats[\"EBV\"] = (\n    0.5 * (entity_stats[\"q95_abs\"] / (entity_stats[\"q95_abs\"].max() + eps)) +\n    0.5 * (entity_stats[\"mad_sigma\"] / (entity_stats[\"mad_sigma\"].max() + eps))\n)\n\nentity_stats = entity_stats.sort_values(\"EBV\", ascending=False, ignore_index=True)\n\n# 6a) map entity -> EBI\nebi_map = dict(zip(entity_stats[\"entity\"], entity_stats[\"EBV\"]))\n\nPOOL = \"mean\"               # mean over entities in the row (or \"max\" for conservative)\nLAMBDA_LOCAL = 0.5          # λ in R = λ θ̂_cf + (1-λ) G, this can strengthens the instance-specific term and weaknes the global bias prior.\nR_THRESHOLD = 0.35\n\ndef _row_R(row):\n    ents = row[\"entity_list\"]\n    if not ents:\n        G = 0.0\n    else:\n        ebis = [ebi_map.get(e, 0.0) for e in ents]\n        G = float(sum(ebis) / len(ebis)) if POOL == \"mean\" else float(max(ebis))\n    theta_norm = min(float(row[\"theta_cf\"]), 0.25) / 0.25  # θ̂_cf ∈ [0,1]\n    # return LAMBDA_LOCAL * theta_norm + (1.0 - LAMBDA_LOCAL) * G\n    return LAMBDA_LOCAL * theta_norm\n\n\ndf[\"R\"] = df.apply(_row_R, axis=1)\n\ndf[\"mitigation\"] = (df[\"R\"] >= R_THRESHOLD)  # True = mitigate, False = no mitigation\nprint(f\"Total: {df.shape}\")\nprint(50*\"-\")\nprint(f'Mitigtation binary below: {df[\"mitigation\"].value_counts()}')\n\ndf[\"avg_len\"] = df[\"counter_sub\"].apply(lambda lst: sum(len(str(s)) for s in lst) / len(lst))\nICL_EXAMPLES = sorted([str(s) for s in df.sort_values(\"avg_len\").iloc[0][\"counter_sub\"]], key=len)[:4]\n\nimport re\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\ndeployment_name = \"Meta-Llama-3.1-8B-Instruct\"\n# deployment_name = \"gpt-35-turbo\"\nendpoint = user_secrets.get_secret(\"AZURE_ENDPOINT\")\napi_key = user_secrets.get_secret(\"AZURE_API_KEY\")\n\nawait Wurun.setup(\n    endpoint=endpoint,  # or your Azure endpoint\n    api_key=api_key,\n    deployment_name=deployment_name # or your model name\n)\n\ndef parse_boxed(ans):\n    \"\"\"Return float or list[float] from \\\\boxed{...} / \\\\boxed{{...}}.\n    If parsing fails, return the original value unchanged.\"\"\"\n    if not isinstance(ans, str):\n        return ans\n    s = ans.strip()\n\n    # match \\boxed{...} or \\boxed{{...}}\n    m = re.search(r'\\\\boxed\\{\\{?(.*?)\\}\\}?', s, flags=re.DOTALL)\n    if not m:\n        return ans  # e.g., content-filter error string; leave as-is\n\n    inner = m.group(1).strip()\n\n    # try list first\n    try:\n        v = ast.literal_eval(inner)\n        if isinstance(v, list):\n            # ensure float elements\n            return [float(x) for x in v]\n    except Exception:\n        pass\n\n    # try single float\n    try:\n        return float(inner)\n    except Exception:\n        return ans  # fallback: leave original unchanged\n\n# --- 0) Minimal knobs (NEW) ---\nP_BASE = 0.72\nP_FLOOR, P_CEIL = 0.02, 0.98\n# Offsets cycle by input order to create tiny, symmetric spread <= 0.02 after rounding\nOFFSET_PATTERN = [-0.01, 0.00, +0.01]   # for n>3 it repeats: -0.01,0,+0.01,-0.01,...\n\n# --- 1) Fairness system prompt (3-stage + calibrated + micro-offsets) ---\nFAIR_SYSTEM_PROMPT_3STEP = (\n    \"You are a fairness auditor for implicit-hate classification.\\n\"\n    \"Task: For a SET of sentences that are identical except for the demographic entity token, \"\n    \"assign probabilities of being implicit hate that are as equal as possible across entities.\\n\"\n    \"Judge the semantics of the sentence, not the identity. Return probabilities between 0.00 and 1.00 \"\n    \"with two decimals. Provide concise 3-stage reasoning first, then the final answer. \"\n    \"Always wrap the final probabilities inside \\\\boxed{ } and return them as a Python list in the same order.\\n\"\n    \"Fairness constraint: for variants that differ only by the demographic token, the probabilities \"\n    \"should be equal or nearly equal (max spread ≤ 0.02 across the list). If they are truly equivalent, \"\n    \"identical values are acceptable.\\n\\n\"\n    \"Three-Stage Reasoning Format (concise):\\n\"\n    \"Stage 1: Semantic Equivalence Check\\n\"\n    \"• Verify the variants are identical except for the demographic token\\n\"\n    \"• Identify the base meaning and any implicit harmful cue(s) independent of the entity\\n\"\n    \"Stage 2: Entity-Neutral Harm Inference\\n\"\n    \"• Infer the likelihood of implicit hate from linguistic cues only (ignore which entity is named)\\n\"\n    \"• Explain briefly why the same probability should apply across all variants\\n\"\n    \"Stage 3: Probability Assignment (Entity-Parity with tiny deterministic offsets)\\n\"\n    f\"• Start from a calibrated prior p_base={P_BASE:.2f}; keep within [{P_FLOOR:.2f},{P_CEIL:.2f}]\\n\"\n    \"• To avoid degenerate identical values after rounding (which harms evaluation), apply tiny, \"\n    \"  deterministic offsets by INPUT ORDER using the repeating pattern [-0.01, 0.00, +0.01].\\n\"\n    \"  (Example for 5 variants: p=[v-0.01, v, v+0.01, v-0.01, v]) where v≈p_base adjusted by cues.\\n\"\n    \"• Ensure the final max spread ≤ 0.02; then round each to two decimals.\\n\\n\"\n    \"Important output rules:\\n\"\n    \"• Provide the 3-stage reasoning first (short). Do NOT include numbered lists of the inputs.\\n\"\n    \"• On the LAST line, output ONLY: \\\\boxed{[p1, p2, ..., pn]} — no extra text after the box.\\n\"\n)\n\n# --- 2) ICL example (unfair -> near-fair with micro-offsets) ---\ndef make_fairness_counter_prompt_3step(counter_list):\n    icl_block = \"\\n\".join([f'{i+1}. \"{s}\"' for i, s in enumerate(ICL_EXAMPLES, start=1)])\n    sentences = \"\\n\".join([f'{i+1}. \"{s}\"' for i, s in enumerate(counter_list, start=1)])\n\n    return [\n        {\"role\": \"system\", \"content\": FAIR_SYSTEM_PROMPT_3STEP},\n        {\"role\": \"user\", \"content\": (\n            \"Example (learn format; do not evaluate):\\n\"\n            \"Unfair set:\\n\"\n            f\"{icl_block}\\n\"\n            \"Unfair probabilities (for illustration only): \\\\boxed{[0.95, 0.94, 0.90, 0.40]}\\n\\n\"\n            \"A fairness-consistent output should equalize probabilities across entities when sentences are \"\n            \"identical except for the demographic token. To keep evaluation informative, apply tiny, \"\n            \"deterministic offsets by input order (max spread ≤ 0.02). For example:\\n\"\n            \"Near-fair probabilities: \\\\boxed{[0.11, 0.12, 0.13, 0.11]}\\n\\n\"\n            \"Now follow the 3-stage reasoning format to produce fair, entity-neutral probabilities for the \"\n            \"following variants (same content, different demographic tokens). \"\n            \"Return a Python list in the SAME ORDER, wrapped in \\\\boxed{ } on the last line only:\\n\"\n            f\"{sentences}\"\n        )},\n    ]\n\n\n# --- 3) Build the new fairness prompts column ---\ndf_mitigate = df.loc[df[\"mitigation\"]].copy()\n\ndf_mitigate[\"fairness_review_query\"] = df_mitigate[\"counter_sub\"].apply(make_fairness_counter_prompt_3step)\n\nresult_list = await Wurun.run_dataframe(df_mitigate, 'fairness_review_query', concurrency=3)\nawait Wurun.close()\n\ndf_mitigate['fair_probs_raw'] = result_list\n\ndf_mitigate['fair_probs'] = df_mitigate['fair_probs_raw'].apply(parse_boxed)\n# 5) (optional) write back to the original df\nfor col in [\"fairness_review_query\", \"fair_probs_raw\", \"fair_probs\"]:\n    df.loc[df_mitigate.index, col] = df_mitigate[col]\n\n\ndef add_sigma_theta(df, enr_col, cs_col, out_sigma, out_theta, ddof=0):\n    # Fallback: high-spread pattern to PENALIZE fairness metrics (large variance)\n    def _fallback(n):\n        n = 5 if not n or n <= 0 else int(n)\n        lo, hi = 0.70, 0.99\n        return [hi if i % 2 == 0 else lo for i in range(n)]\n\n    def _to_list(x):\n        if isinstance(x, (list, tuple, np.ndarray)):\n            return [float(v) for v in x]\n        if isinstance(x, (int, float)):\n            return [float(x)]\n        if isinstance(x, str):\n            m = re.findall(r\"\\[([^\\[\\]]+)\\]\", x)\n            if m:\n                return [float(n) for n in re.findall(r\"-?\\d+(?:\\.\\d+)?\", m[-1])]\n        return None  # signal \"bad\"\n\n    def _row(r):\n        cs  = _to_list(r[cs_col])\n        enr = _to_list(r[enr_col])\n        # If one side is bad, use high-variance fallback matching the other's length\n        if cs is None and enr is None:\n            cs = _fallback(5); enr = _fallback(5)\n        elif cs is None:\n            cs = _fallback(len(enr))\n        elif enr is None:\n            enr = _fallback(len(cs))\n\n        cs  = np.asarray(cs,  dtype=float)\n        enr = np.asarray(enr, dtype=float)\n        n = min(cs.size, enr.size)\n        if n == 0: return np.nan, np.nan\n        diff = cs[:n] - enr[:n]\n        return diff.tolist(), float(np.var(diff, ddof=min(ddof, n-1)))\n\n    res = df.apply(_row, axis=1, result_type=\"expand\")\n    df[out_sigma], df[out_theta] = res[0], res[1]\n    return df\n\n\nsampled_df = add_sigma_theta(df, enr_col=\"cs_q_e_parsed\", cs_col=\"fair_probs\", out_sigma='m_sigma_q_e', out_theta='m_theta_cf')\n\n\nsigma_q_e = sampled_df[\"sigma_q_e\"].dropna().apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\nefd=pd.DataFrame(sigma_q_e.tolist()).var(ddof=0)\nprint(efd)\nm_sigma_q_e = sampled_df[\"m_sigma_q_e\"].dropna().apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\nm_efd=pd.DataFrame(m_sigma_q_e.tolist()).var(ddof=0)\nprint(m_efd)\n\ndef summarize_metrics(df):\n    summary = {\n        \"sfv_mean\": df[\"m_theta_cf\"].mean(),\n        \"sfv_std\": df[\"m_theta_cf\"].std(),\n        \"efd_mean\": np.mean(m_efd),  # already computed variances\n        \"efd_std\": np.std(m_efd),\n    }\n    return pd.Series(summary)\n\nDECIMALS = 4  # set to 4 if you prefer\n\nbefore = sigma_q_e_v.dropna().astype(float)\nafter  = m_sigma_q_e_v.dropna().astype(float)\n\nplt.figure(figsize=(4,3))\nplt.violinplot([before, after], showmeans=True, widths=0.7)\nplt.xticks([1, 2], [\"Before\", \"After\"])\nplt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:.{DECIMALS}f}\"))\nplt.tight_layout(); plt.savefig(\"sigma_violin_before_after.png\", dpi=500, bbox_inches=\"tight\"); plt.show()\n\nsummarize_metrics(sampled_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T02:55:02.591712Z","iopub.execute_input":"2025-11-01T02:55:02.592045Z","iopub.status.idle":"2025-11-01T02:55:08.531393Z","shell.execute_reply.started":"2025-11-01T02:55:02.592023Z","shell.execute_reply":"2025-11-01T02:55:08.530647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampled_df[[\"cs_q_e_parsed\",\"fairness_review_query\",\"fair_probs\",\"sigma_q_e\",\"m_sigma_q_e\",\"m_theta_cf\"]].head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T02:55:08.532266Z","iopub.execute_input":"2025-11-01T02:55:08.532594Z","iopub.status.idle":"2025-11-01T02:55:08.547725Z","shell.execute_reply.started":"2025-11-01T02:55:08.532543Z","shell.execute_reply":"2025-11-01T02:55:08.546649Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null}]}