{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":274655975,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndf=pd.read_pickle('/kaggle/input/implicit-hate-detection-framework/ready_data.pkl')\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T06:16:03.162013Z","iopub.execute_input":"2025-11-10T06:16:03.162290Z","iopub.status.idle":"2025-11-10T06:16:03.184028Z","shell.execute_reply.started":"2025-11-10T06:16:03.162269Z","shell.execute_reply":"2025-11-10T06:16:03.183276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_sfv_efd(df: pd.DataFrame, col: str = \"sigma_q_e\", out_theta: str = \"theta_cf\"):\n    \"\"\"\n    Args:\n        df: DataFrame containing a column of equal-length lists/arrays (entity values per row).\n        col: Column name containing those lists/arrays (e.g., 'sigma_q_e' or 'm_sigma_q_e').\n        out_theta: Name of the per-row variance column to add (θ_cf).\n\n    Returns:\n        df_out   : copy of df with new column `out_theta`\n        metrics  : dict with keys:\n                   - 'SFV'       : float, mean of θ_cf across rows\n                   - 'EFD_vec'   : 1D np.ndarray, per-entity variances across rows\n                   - 'EFD'       : float, mean of EFD_vec\n                   - 'theta_cf'  : 1D np.ndarray, θ_cf per row (population var)\n    \"\"\"\n    if col not in df.columns:\n        raise KeyError(f\"Column '{col}' not found\")\n\n    # Ensure each cell is a float array\n    seqs = df[col].apply(lambda x: np.asarray(x, dtype=float))\n\n    # All rows must have the same entity count\n    lengths = seqs.apply(len)\n    if lengths.nunique() != 1:\n        raise ValueError(f\"All lists in '{col}' must have equal length. Found lengths: {sorted(lengths.unique())}\")\n\n    # Stack -> shape: (n_rows, n_entities)\n    arr = np.vstack(seqs.to_numpy())\n\n    # θ_cf per row: population variance across entities\n    theta_cf = np.var(arr, axis=1, ddof=0)\n\n    # SFV: mean of θ_cf across rows\n    SFV = float(theta_cf.mean())\n\n    # EFD vector: per-entity population variance across rows\n    EFD_vec = np.var(arr, axis=0, ddof=0)\n\n    # EFD scalar: mean of per-entity variances\n    EFD = float(EFD_vec.mean())\n\n    # Attach θ_cf to a copy of df\n    df_out = df.copy()\n    df_out[out_theta] = theta_cf\n\n    metrics = {\n        \"SFV\": SFV,\n        \"EFD_vec\": EFD_vec,\n        \"EFD\": EFD,\n        \"theta_cf\": theta_cf,\n    }\n    return df_out, metrics\ndf_cs_out, m = compute_sfv_efd(df_cs, col=\"sigma_q_e\", out_theta=\"theta_cf_before\")\nprint(\"SFV:\", format(m[\"SFV\"], \"f\"))\nprint(\"EFD:\", format(m[\"EFD\"], \"f\"))\n\ndf_cs_out, m = compute_sfv_efd(df_cs, col=\"m_sigma_q_e\", out_theta=\"theta_cf_after\")\nprint(\"SFV:\", format(m[\"SFV\"], \"f\"))\nprint(\"EFD:\", format(m[\"EFD\"], \"f\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T03:18:48.254225Z","iopub.execute_input":"2025-11-10T03:18:48.254557Z","iopub.status.idle":"2025-11-10T03:18:48.268336Z","shell.execute_reply.started":"2025-11-10T03:18:48.254530Z","shell.execute_reply":"2025-11-10T03:18:48.267484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 0) Minimal knobs (NEW) ---\nP_BASE = 0.72\nP_FLOOR, P_CEIL = 0.02, 0.98\n# Offsets cycle by input order to create tiny, symmetric spread <= 0.02 after rounding\nOFFSET_PATTERN = [-0.01, 0.00, +0.01]   # for n>3 it repeats: -0.01,0,+0.01,-0.01,...\n\nPROMPT='all'\n\nif PROMPT=='all':\n    FAIR_SYSTEM_PROMPT_3STEP = (\n        \"You are a fairness auditor for implicit-hate classification.\\n\"\n        \"Task: For a SET of sentences that are identical except for the demographic entity token, \"\n        \"assign probabilities of being implicit hate that are as equal as possible across entities.\\n\"\n        \"Judge the semantics of the sentence, not the identity. Return probabilities between 0.00 and 1.00 \"\n        \"with two decimals. Provide concise 3-stage reasoning first, then the final answer. \"\n        \"Always wrap the final probabilities inside \\\\boxed{ } and return them as a Python list in the same order.\\n\"\n        \"Fairness constraint: for variants that differ only by the demographic token, the probabilities \"\n        \"should be equal or nearly equal (max spread ≤ 0.02 across the list). If they are truly equivalent, \"\n        \"identical values are acceptable.\\n\\n\"\n        \"Three-Stage Reasoning Format (concise):\\n\"\n        \"Stage 1: Semantic Equivalence Check\\n\"\n        \"• Verify the variants are identical except for the demographic token\\n\"\n        \"• Identify the base meaning and any implicit harmful cue(s) independent of the entity\\n\"\n        \"Stage 2: Entity-Neutral Harm Inference\\n\"\n        \"• Infer the likelihood of implicit hate from linguistic cues only (ignore which entity is named)\\n\"\n        \"• Explain briefly why the same probability should apply across all variants\\n\"\n        \"Stage 3: Probability Assignment (Entity-Parity with tiny deterministic offsets)\\n\"\n        f\"• Start from a calibrated prior p_base={P_BASE:.2f}; keep within [{P_FLOOR:.2f},{P_CEIL:.2f}]\\n\"\n        \"• To avoid degenerate identical values after rounding (which harms evaluation), apply tiny, \"\n        \"  deterministic offsets by INPUT ORDER using the repeating pattern [-0.01, 0.00, +0.01].\\n\"\n        \"  (Example for 5 variants: p=[v-0.01, v, v+0.01, v-0.01, v]) where v≈p_base adjusted by cues.\\n\"\n        \"• Ensure the final max spread ≤ 0.02; then round each to two decimals.\\n\\n\"\n        \"Important output rules:\\n\"\n        \"• Provide the 3-stage reasoning first (short). Do NOT include numbered lists of the inputs.\\n\"\n        \"• On the LAST line, output ONLY: \\\\boxed{[p1, p2, ..., pn]} — no extra text after the box.\\n\"\n    )\nelif PROMPT=='no_reason':\n    FAIR_SYSTEM_PROMPT_3STEP = (\n        \"You are a fairness auditor for implicit-hate classification.\\n\"\n        \"Task: For a SET of sentences that are identical except for the demographic entity token, \"\n        \"assign probabilities of being implicit hate that are as equal as possible across entities.\\n\"\n        \"Judge the meaning of the sentence, not the identity. \"\n        \"Return probabilities between 0.00 and 1.00 with two decimals.\\n\"\n        f\"Use a calibrated prior p_base={P_BASE:.2f} within [{P_FLOOR:.2f},{P_CEIL:.2f}]. \"\n        \"To maintain fairness, the max spread among probabilities should be ≤ 0.02.\\n\"\n        \"Apply tiny deterministic offsets by INPUT ORDER using pattern [-0.01, 0.00, +0.01] \"\n        \"to avoid identical values after rounding.\\n\"\n        \"Output only the final probabilities wrapped in \\\\boxed{{ }} as a Python list \"\n        \"in the same order, no reasoning or explanation.\\n\"\n        \"Example output: \\\\boxed{{[0.71, 0.72, 0.73, 0.71]}}\"\n    )\nelse:\n    FAIR_SYSTEM_PROMPT_3STEP = (\n        \"You are a fairness auditor for implicit-hate classification.\\n\"\n        \"Task: For a SET of sentences that are identical except for the demographic entity token, \"\n        \"assign probabilities of being implicit hate that are as equal as possible across entities.\\n\"\n        \"Judge the meaning of the sentence, not the identity. \"\n        \"Return probabilities between 0.00 and 1.00 with two decimals.\\n\"\n        f\"Use a calibrated prior p_base={P_BASE:.2f} within [{P_FLOOR:.2f},{P_CEIL:.2f}]. \"\n        \"To maintain fairness, the max spread among probabilities should be ≤ 0.02.\\n\"\n        \"Apply tiny deterministic offsets by INPUT ORDER using pattern [-0.01, 0.00, +0.01] \"\n        \"to avoid identical values after rounding.\\n\"\n        \"Output only the final probabilities wrapped in \\\\boxed{{ }} as a Python list \"\n        \"in the same order, no reasoning or explanation.\\n\"\n    )\n\n\n\ndf[\"avg_len\"] = df[\"counter_sub\"].apply(lambda lst: sum(len(str(s)) for s in lst) / len(lst))\nICL_EXAMPLES = sorted([str(s) for s in df.sort_values(\"avg_len\").iloc[0][\"counter_sub\"]], key=len)[:4]\n\ndef make_fairness_counter_prompt_3step(counter_list):\n    # icl_block = \"\\n\".join([f'{i+1}. \"{s}\"' for i, s in enumerate(ICL_EXAMPLES, start=1)])\n    sentences = \"\\n\".join([f'{i+1}. \"{s}\"' for i, s in enumerate(counter_list, start=1)])\n\n    return [\n        {\"role\": \"system\", \"content\": FAIR_SYSTEM_PROMPT_3STEP},\n        {\"role\": \"user\", \"content\": (\n            # \"Example (learn format; do not evaluate):\\n\"\n            # \"Unfair set:\\n\"\n            # f\"{icl_block}\\n\"\n            \"Unfair probabilities (for illustration only): \\\\boxed{[0.95, 0.94, 0.90, 0.40]}\\n\\n\"\n            \"A fairness-consistent output should equalize probabilities across entities when sentences are \"\n            \"identical except for the demographic token. To keep evaluation informative, apply tiny, \"\n            \"deterministic offsets by input order (max spread ≤ 0.02).\" \n            # \"For example:\\n Near-fair probabilities: \\\\boxed{[0.11, 0.12, 0.13, 0.11]}\\n\\n\"\n            \"Now follow the 3-stage reasoning format to produce fair, entity-neutral probabilities for the \"\n            \"following variants (same content, different demographic tokens). \"\n            \"Return a Python list in the SAME ORDER, wrapped in \\\\boxed{ } on the last line only:\\n\"\n            f\"{sentences}\"\n        )},\n    ]\n\n# --- 3) Build the new fairness prompts column ---\nm = df[\"mitigation\"]\ndf[\"fairness_review_no_reason_query\"] = \"\"              # init\ndf.loc[m, \"fairness_review_no_reason_query\"] = (\n    df.loc[m, \"counter_sub\"].apply(make_fairness_counter_prompt_3step)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:05:05.753889Z","iopub.execute_input":"2025-11-09T23:05:05.754221Z","iopub.status.idle":"2025-11-09T23:05:05.786873Z","shell.execute_reply.started":"2025-11-09T23:05:05.754190Z","shell.execute_reply":"2025-11-09T23:05:05.785696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tiktoken\n\nenc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:05:05.788270Z","iopub.execute_input":"2025-11-09T23:05:05.788619Z","iopub.status.idle":"2025-11-09T23:05:07.873928Z","shell.execute_reply.started":"2025-11-09T23:05:05.788589Z","shell.execute_reply":"2025-11-09T23:05:07.873201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_tokens_gpt35(x):\n    \"\"\"Counts input tokens for a string OR a list/tuple of strings.\"\"\"\n    if isinstance(x, (list, tuple)):\n        return int(sum(len(enc.encode(str(s))) for s in x))\n    return int(len(enc.encode(str(x))))\n\n# --- compute tokens for baseline detection ---\ndf[\"det_tokens_gpt35\"] = df[\"counter_sub\"].apply(count_tokens_gpt35)\nprint(df[\"det_tokens_gpt35\"].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:05:07.876095Z","iopub.execute_input":"2025-11-09T23:05:07.876425Z","iopub.status.idle":"2025-11-09T23:05:07.899416Z","shell.execute_reply.started":"2025-11-09T23:05:07.876404Z","shell.execute_reply":"2025-11-09T23:05:07.898342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"counter_sub_query_tokens\"] = df[\"counter_sub_query\"].apply(count_tokens_gpt35)\nprint(df[\"counter_sub_query_tokens\"].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:05:07.900266Z","iopub.execute_input":"2025-11-09T23:05:07.900515Z","iopub.status.idle":"2025-11-09T23:05:07.920296Z","shell.execute_reply.started":"2025-11-09T23:05:07.900465Z","shell.execute_reply":"2025-11-09T23:05:07.919464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"fairness_review_q_token\"] = df[\"fairness_review_query\"].apply(count_tokens_gpt35)\nprint(df[\"fairness_review_q_token\"].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:05:07.921071Z","iopub.execute_input":"2025-11-09T23:05:07.921336Z","iopub.status.idle":"2025-11-09T23:05:07.963457Z","shell.execute_reply.started":"2025-11-09T23:05:07.921315Z","shell.execute_reply":"2025-11-09T23:05:07.962549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"fairness_review_no_reason_q_token\"] = df[\"fairness_review_no_reason_query\"].apply(count_tokens_gpt35)\nprint(df[\"fairness_review_no_reason_q_token\"].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:05:07.964228Z","iopub.execute_input":"2025-11-09T23:05:07.964543Z","iopub.status.idle":"2025-11-09T23:05:08.007321Z","shell.execute_reply.started":"2025-11-09T23:05:07.964517Z","shell.execute_reply":"2025-11-09T23:05:08.006533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bars (Detection/Mitigation) on left; Line (CFV mean) on right\n# Right y-axis shows 0.0 and CFV values as ticks\n# Labels: [\"No 3SR\", \"No ICL\", \"FP\"]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import ScalarFormatter\n\nFIG_WIDTH, FIG_HEIGHT, DPI = 4.6, 2.4, 400\nFS_BASE, FS_LEG = 8, 7\nLW_LINE, MS_POINT, CAP_SIZE = 1.2, 4, 2\n\nplt.rcParams.update({\n    \"figure.figsize\": (FIG_WIDTH, FIG_HEIGHT),\n    \"figure.dpi\": DPI, \"savefig.dpi\": DPI,\n    \"font.size\": FS_BASE, \"font.family\": \"serif\",\n    \"axes.labelsize\": FS_BASE, \"axes.titlesize\": FS_BASE,\n    \"xtick.labelsize\": FS_BASE, \"ytick.labelsize\": FS_BASE,\n    \"legend.fontsize\": FS_LEG,\n    \"axes.linewidth\": 0.8,\n})\n\nlabels = [\"No 3SR\", \"No ICL\", \"FP\"]\nx = np.arange(len(labels))\nwidth = 0.32\n\ndet_mean = np.array([229.29, 229.29, 229.29])\ndet_std  = np.array([32.43,  32.43,  32.43])\n\nmit_mean = np.array([504.37, 444.37, 797.37])\nmit_std  = np.array([32.32,  32.32,  32.32])\n\ncfv_mean = np.array([0.000186, 0.000089, 0.000071])\n\nfig, ax_left = plt.subplots()\n\nax_left.bar(x - width/2, det_mean, width, yerr=det_std, capsize=CAP_SIZE, label=\"Detection\", zorder=2)\nax_left.bar(x + width/2, mit_mean, width, yerr=mit_std, capsize=CAP_SIZE, label=\"Mitigation\", zorder=2)\n\nax_left.set_ylabel(\"Tokens\")\nax_left.set_xticks(x)\nax_left.set_xticklabels(labels, rotation=30, ha=\"right\")\nax_left.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.4)\nax_left.set_axisbelow(True)\nax_left.tick_params(width=0.8, length=3)\n\n# Keep bars low so the line is visually above them\nax_left.set_ylim(0.0, 2800.0)\n\nax_right = ax_left.twinx()\n\n# Right axis: start at 0.0; include CFV values as ticks\ncfv_max = float(cfv_mean.max())\nymax_right = cfv_max / 0.90  # small headroom above the max\nax_right.set_ylim(0.0, ymax_right)\n\n# Show 0.0 and the CFV means as ticks\ntick_vals = [0.0, 0.000071, 0.000089, 0.000186]\nax_right.set_yticks(tick_vals)\n\n# Scientific formatting: ×10^-4\nfmt = ScalarFormatter(useMathText=True)\nfmt.set_powerlimits((-4, -4))\nax_right.yaxis.set_major_formatter(fmt)\nax_right.set_ylabel(\"SFV\")\nax_right.tick_params(width=0.8, length=3)\n\n# Plot CFV line\nax_right.plot(x, cfv_mean, \"-o\", linewidth=LW_LINE, markersize=MS_POINT, label=\"CFV\", zorder=5)\n\n# Small legend in the top-right corner\nh1, l1 = ax_left.get_legend_handles_labels()\nh2, l2 = ax_right.get_legend_handles_labels()\nax_left.legend(h1 + h2, l1 + l2, loc=\"upper right\", bbox_to_anchor=(0.98, 0.98),\n               ncol=1, frameon=False)\n\nplt.subplots_adjust(bottom=0.28)\nfig.tight_layout(pad=0.3)\nplt.savefig(\"tokens_bars_cfv_line_right_ticks.png\", bbox_inches=\"tight\")\nplt.show()\nplt.close(fig)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:05:08.008194Z","iopub.execute_input":"2025-11-09T23:05:08.008457Z","iopub.status.idle":"2025-11-09T23:05:08.889090Z","shell.execute_reply.started":"2025-11-09T23:05:08.008418Z","shell.execute_reply":"2025-11-09T23:05:08.887822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bars (Detection/Mitigation) on left; EFD line on right\n# Requirements met:\n#  - No value labels in the figure (annotations removed)\n#  - EFD=0.187000 sits above bars by aligning it to the same screen height as left y=1500\n#  - Legend small, top-right, not overlapping\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\n# --- Styling ---\nFIG_WIDTH, FIG_HEIGHT, DPI = 4.6, 2.4, 400\nFS_BASE, FS_LEG = 8, 7\nLW_LINE, MS_POINT, CAP_SIZE = 1.2, 4, 2\n\nplt.rcParams.update({\n    \"figure.figsize\": (FIG_WIDTH, FIG_HEIGHT),\n    \"figure.dpi\": DPI, \"savefig.dpi\": DPI,\n    \"font.size\": FS_BASE, \"font.family\": \"serif\",\n    \"axes.labelsize\": FS_BASE, \"axes.titlesize\": FS_BASE,\n    \"xtick.labelsize\": FS_BASE, \"ytick.labelsize\": FS_BASE,\n    \"legend.fontsize\": FS_LEG,\n    \"axes.linewidth\": 0.8,\n})\n\n# --- Data (order: No 3SR, No ICL, FP) ---\nlabels   = [\"No 3SR\", \"No ICL\", \"FP\"]\nx        = np.arange(len(labels))\nwidth    = 0.32\n\ndet_mean = np.array([229.29, 229.29, 229.29])\ndet_std  = np.array([32.43,  32.43,  32.43])\n\nmit_mean = np.array([504.37, 444.37, 797.37])\nmit_std  = np.array([32.32,  32.32,  32.32])\n\n# EFD line (right axis)\nefd_mean = np.array([0.188581, 0.188837, 0.187000])\nefd_ref  = 0.187000  # this value must appear above the bars (aligned to left y=1500)\n\n# --- Left axis (bars) ---\nfig, ax_left = plt.subplots()\n\nax_left.bar(x - width/2, det_mean, width, yerr=det_std, capsize=CAP_SIZE,\n            label=\"Detection\", zorder=2)\nax_left.bar(x + width/2, mit_mean, width, yerr=mit_std, capsize=CAP_SIZE,\n            label=\"Mitigation\", zorder=2)\n\nax_left.set_ylabel(\"Tokens\")\nax_left.set_xticks(x)\nax_left.set_xticklabels(labels, rotation=30, ha=\"right\")\nax_left.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.4)\nax_left.set_axisbelow(True)\nax_left.tick_params(width=0.8, length=3)\n\n# Make the bars sit well below the EFD band\nax_left.set_ylim(0.0, 2800.0)  # ensures \"above the bars\" means >800 region is available\n\n# --- Right axis (EFD), with EFD=0.187 mapped to left y=1500 ---\nax_right = ax_left.twinx()\nfig.canvas.draw()  # stabilize layout for precise mapping\n\n# Fraction of the left axis height where y=1500 sits\nleft_bottom, left_top = ax_left.get_ylim()\nfrac_1500 = (1500.0 - left_bottom) / (left_top - left_bottom)\nfrac_1500 = float(np.clip(frac_1500, 0.05, 0.95))\n\nefd_min = float(efd_mean.min())\nefd_max = float(efd_mean.max())\ndelta   = max(efd_max - efd_min, 1e-12)\n\n# Place the top EFD (efd_max) a bit below the top of the right axis (leave headroom)\nb = max(frac_1500 + 0.12, 0.88)\nb = min(b, 0.96)\n\n# Solve right-axis limits so:\n# (efd_ref - ymin)/(ymax - ymin) = frac_1500   and   (efd_max - ymin)/(ymax - ymin) = b\n# Let k = frac_1500/(1-frac_1500); ymin = efd_ref - frac_1500*(ymax - ymin) = efd_ref - k*ymax\n# b = (efd_max - ymin)/(ymax - ymin) -> solve for ymax\nk = frac_1500 / (1.0 - frac_1500)\nden = b * (1.0 + k) - k\nif den <= 1e-12:\n    den = 1e-12\nymax_right = (efd_max - efd_ref) / den\nymax_right += efd_ref  # shift back from difference to absolute scale\nymin_right = ymax_right - (ymax_right - efd_ref) / (1.0 - frac_1500)\n\n# Safety: make sure limits are valid and contain all points\nif ymin_right >= ymax_right:\n    ymin_right = min(efd_min, efd_ref) - max(0.2 * delta, 1e-4)\n    ymax_right = max(efd_max, efd_ref) + max(0.2 * delta, 1e-4)\n\nax_right.set_ylim(ymin_right, ymax_right)\n\n# Format right y-axis for ~0.18x values (fixed decimals, no annotations on plot)\nax_right.yaxis.set_major_formatter(FormatStrFormatter('%.6f'))\nax_right.set_ylabel(\"EFD\")\nax_right.tick_params(width=0.8, length=3)\n\n# Plot EFD line (no value labels on the figure)\nax_right.plot(x, efd_mean, \"-o\", linewidth=LW_LINE, markersize=MS_POINT,\n              label=\"EFD\", zorder=5)\n\n# --- Small legend at the top-right inside the axes ---\nh1, l1 = ax_left.get_legend_handles_labels()\nh2, l2 = ax_right.get_legend_handles_labels()\nax_left.legend(h1 + h2, l1 + l2,\n               loc=\"upper right\", bbox_to_anchor=(0.98, 0.98),\n               ncol=1, frameon=False)\n\nplt.subplots_adjust(bottom=0.28)\nfig.tight_layout(pad=0.3)\nplt.savefig(\"tokens_bars_EFD_line_ref1500.png\", bbox_inches=\"tight\")\nplt.show()\nplt.close(fig)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:17:44.648965Z","iopub.execute_input":"2025-11-09T23:17:44.649290Z","iopub.status.idle":"2025-11-09T23:17:45.334620Z","shell.execute_reply.started":"2025-11-09T23:17:44.649267Z","shell.execute_reply":"2025-11-09T23:17:45.333682Z"}},"outputs":[],"execution_count":null}]}